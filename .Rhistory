most_freq_topic_polylas_il<-which.max(tabulate(topics1_polylas_il))
topic_terms_polylas_il<-as.data.frame(terms(polylas_il_LDA2, 10))
terms2_polylas_il<-terms(polylas_il_LDA2, 5)
theta_polylas_il<-as.data.frame(posterior(polylas_il_LDA2)$topics)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
polylas_il_LDA2_json<-topicmodels_json_ldavis(polylas_il_LDA2, polylas_il, polylas_il_dtm)
serVis(polylas_il_LDA2_json)
s_theta_pers_il<-apply(theta_pers_il, 1, sort)
s_theta_polylas_il<-apply(theta_polylas_il, 1, sort)
Diff_pers_polylas<-(sqrt(s_theta_pers_il)-sqrt(s_theta_polylas_il))**2
HD_pers_polylas0<-rowSums(Diff_pers_polylas)
HD_pers_polylas<-mean(HD_pers_polylas0)
s_theta_kazatzakis_il<-apply(theta_kazatzakis_il, 1, sort)
Diff_pers_kazatzakis<-(sqrt(s_theta_pers_il)-sqrt(s_theta_kazatzakis_il))**2
HD_pers_kazatzakis0<-rowSums(Diff_pers_kazatzakis)
HD_pers_kazatzakis<-mean(HD_pers_kazatzakis0)
library(NLP)
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
butler_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Butler_1898"), readerControl=list(language="en"))
inspect(butler_il[[1]])
butler_il.trans1<-tm_map(butler_il, removeNumbers)
butler_il.trans2<-tm_map(butler_il.trans1, removePunctuation)
butler_il.trans3<-tm_map(butler_il.trans2, stripWhitespace)
butler_il_dtm<-DocumentTermMatrix(butler_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
butler_il_LDA2 <- LDA(butler_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_butler_il<-topics(butler_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_butler_il
most_freq_topic_butler_il<-which.max(tabulate(topics1_butler_il))
topic_terms_butler_il<-as.data.frame(terms(butler_il_LDA2, 10))
terms2_butler_il<-terms(butler_il_LDA2, 5)
terms2_butler_il
theta_butler_il<-as.data.frame(posterior(butler_il_LDA2)$topics)
theta_butler_il
topics2_butler_il<-topics(butler_il_LDA2, 2)
topics2_butler_il
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
butler_il_LDA2_json<-topicmodels_json_ldavis(butler_il_LDA2, butler_il, butler_il_dtm)
serVis(butler_il_LDA2_json)
s_theta_butler_il<-apply(theta_butler_il, 1, sort)
Diff_pers_butler<-(sqrt(s_theta_pers_il)-sqrt(s_theta_butler_il))**2
HD_pers_butler0<-rowSums(Diff_pers_butler)
HD_pers_butler<-mean(HD_pers_butler0)
HD_pers_butler
HD_pers_polylas
HD_pers_kazatzakis
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Polylas_1890_iliad"), readerControl=list(language="el"))
inspect(polylas_il[[1]])
print(polylas_il[[1]])
meta(polylas_il[[1]])
str(polylas_il)
polylas_il.trans1<-tm_map(polylas_il, removeNumbers)
inspect(polylas_il.trans1[[1]])
polylas_il.trans2<-tm_map(polylas_il.trans1, removePunctuation)
inspect(polylas_il.trans2[[1]])
polylas_il.trans3<-tm_map(polylas_il.trans2, stripWhitespace)
inspect(polylas_il.trans3[[1]])
polylas_il_dtm<-DocumentTermMatrix(polylas_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
inspect(polylas_il_dtm)
polylas_il_dtm$v[1:3]
polylas_il_dtm$dimnames$Terms[1:3] #Corpus dictionary: length = 19763
findFreqTerms(polylas_il_dtm, 50) #List of words appearing more than 50 times
dim(polylas_il_dtm)
str(polylas_il_dtm)
nrow(polylas_il_dtm) #number of documents (i.e. books of the Iliad)
ncol(polylas_il_dtm) #number of terms
rownames(polylas_il_dtm)
colnames(polylas_il_dtm) #Alternatively: polylas_il$dimnames$Terms
dictionary_polylas_il<-colnames(polylas_il_dtm)
dictionary_polylas_il
length(dictionary_polylas_il)
library(RColorBrewer)
library(wordcloud)
UnsortedFreq<-colSums(as.matrix(polylas_il_dtm))
UnsortedFreq[1:5]
dictionary_polylas_il[1:5]
wordFreqs_polylas_il<-sort(colSums(as.matrix(polylas_il_dtm)), decreasing=TRUE)
wordFreqs_polylas_il[1:1000]
names(wordFreqs_polylas_il)[1:100]
MostFreqNames<-(wordFreqs_polylas_il)[1:100]
MostFreqNames
wordcloud(words=names(MostFreqNames), freq=MostFreqNames)
library(slam)
tfidf_polylas_il<-tapply(polylas_il_dtm$v / row_sums(polylas_il_dtm)[polylas_il_dtm$i], polylas_il_dtm$j, mean) *
log2(nDocs(polylas_il_dtm)/col_sums(polylas_il_dtm > 0))
is.vector(tfidf_polylas_il) #not a vector
tfidf_polylas_il[1:5]
dictionary_polylas_il[1:5]
length(tfidf_polylas_il)
dim(tfidf_polylas_il)
summary(tfidf_polylas_il)
reduced_dtm_polylas_il<-polylas_il_dtm[,tfidf_polylas_il >= 0.001]
summary(col_sums(reduced_dtm_polylas_il))
inspect(reduced_dtm_polylas_il)
dim(reduced_dtm_polylas_il)
findFreqTerms(reduced_dtm_polylas_il, 5) # at least 5 times
wordFreqs_polylas_il<-sort(colSums(as.matrix(reduced_dtm_polylas_il)), decreasing=TRUE)
wordcloud(words=names(wordFreqs_polylas_il), freq=wordFreqs_polylas_il)
harmonicMean<-function(logLikelihoods, precision = 2000L) {
llMed<-median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
k<-25
burnin<-1000
iter<-1000
keep<-50
polylas_il_LDA<-LDA(polylas_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
polylas_il_LDA
str(polylas_il_LDA)
logLiks_polylas_il<-polylas_il_LDA@logLiks[-c(1:(burnin/keep))]
polylas_il_LDA@logLiks
library(Rmpfr)
library(gmp)
seqk<-seq(2, 50, 1)
harmonicMean(logLiks_polylas_il)
seqk
burnin<-1000
iter<-1000
keep<-50
system.time(fitted_many<-lapply(seqk, function(k) LDA(polylas_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin,
iter = iter, keep = keep) )))
logLiks_many<-lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
hm_many<-sapply(logLiks_many, function(h) harmonicMean(h))
hm_many
which.max(hm_many)
max(hm_many)
seqk
seqk[which.max(hm_many)]
library(ggplot2)
plot(seqk, hm_many)
ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
xlab('Number of Topics') +
ylab('Harmonic Mean') +
annotate("text", x = 5, y = -10000,
label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
ggtitle(expression(atop("polylas_il", atop(italic("How many distinct topics"), ""))))
ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
xlab('Number of Topics') +
ylab('Harmonic Mean') +
annotate("text", x = 5, y = -10000,
label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
ggtitle(expression(atop("polylas_il", atop(italic("How many distinct topics"), ""))))
plot(seqk, hm_many)
s_theta_pallis_il<-apply(theta_pallis_il, 1, sort)
Diff_pers_pallis<-(sqrt(s_theta_pers_il)-sqrt(s_theta_pallis_il))**2
HD_pers_pallis0<-rowSums(Diff_pers_pallis)
HD_pers_pallis<-mean(HD_pers_pallis0)
pers_wout13_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/perseus_iliad_minus_bk13"), readerControl=list(language="en"))
pers_wout13_il.trans1<-tm_map(pers_wout13_il, removeNumbers)
pers_wout13_il.trans2<-tm_map(pers_wout13_il.trans1, removePunctuation)
pers_wout13_il.trans3<-tm_map(pers_wout13_il.trans2, stripWhitespace)
pers_wout13_il_dtm<-DocumentTermMatrix(pers_wout13_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
pers_wout13_il_LDA2 <- LDA(pers_wout13_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pers_wout13_il<-topics(pers_wout13_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_pers_wout13_il
most_freq_topic_pers_wout13_il<-which.max(tabulate(topics1_pers_wout13_il))
topic_terms_pers_wout13_il<-as.data.frame(terms(pers_wout13_il_LDA2, 10))
terms2_pers_wout13_il<-terms(pers_wout13_il_LDA2, 5)
terms2_pers_wout13_il
theta_pers_wout13_il<-as.data.frame(posterior(pers_wout13_il_LDA2)$topics)
theta_pers_wout13_il
topics2_pers_wout13_il<-topics(pers_wout13_il_LDA2, 2)
topics2_pers_wout13_il
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
pers_wout13_il_LDA2_json<-topicmodels_json_ldavis(pers_wout13_il_LDA2, pers_wout13_il, pers_wout13_il_dtm)
serVis(pers_wout13_il_LDA2_json)
View(theta_pers_wout13_il)
s_theta_pers_wout13_il<-apply(theta_pers_wout13_il, 1, sort)
s_theta_pallis_il<-apply(theta_pallis_il, 1, sort)
Diff_pers_wout13_pallis<-(sqrt(s_theta_pers_wout13_il)-sqrt(s_theta_pallis_il))**2
HD_pers_wout13_pallis0<-rowSums(Diff_pers_wout13_pallis)
HD_pers_wout13_pallis<-mean(HD_pers_wout13_pallis0)
HD_pers_wout13_pallis
s_theta_polylas_il<-apply(theta_polylas_il, 1, sort)
s_theta_butler_il<-apply(theta_butler_il, 1, sort)
Diff_polylas_butler<-(sqrt(s_theta_polylas_il)-sqrt(s_theta_butler_il))**2
HD_polylas_butler0<-rowSums(Diff_polylas_butler)
HD_polylas_butler<-mean(HD_polylas_butler0)
HD_polylas_butler
s_theta_kazatzakis_il<-apply(theta_kazatzakis_il, 1, sort)
s_theta_butler_il<-apply(theta_butler_il, 1, sort)
Diff_kazatzakis_butler<-(sqrt(s_theta_kazatzakis_il)-sqrt(s_theta_butler_il))**2
HD_kazatzakis_butler0<-rowSums(Diff_kazatzakis_butler)
HD_kazatzakis_butler<-mean(HD_kazatzakis_butler0)
HD_kazatzakis_butler
s_theta_kazatzakis_il<-apply(theta_kazatzakis_il, 1, sort)
s_theta_polylas_il<-apply(theta_polylas_il, 1, sort)
Diff_kazatzakis_polylas<-(sqrt(s_theta_kazatzakis_il)-sqrt(s_theta_polylas_il))**2
HD_kazatzakis_polylas0<-rowSums(Diff_kazatzakis_polylas)
HD_kazatzakis_polylas<-mean(HD_kazatzakis_polylas0)
HD_kazatzakis_polylas
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
murray_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Murray_1924"), readerControl=list(language="en"))
murray_il.trans1<-tm_map(murray_il, removeNumbers)
murray_il.trans2<-tm_map(murray_il.trans1, removePunctuation)
murray_il.trans3<-tm_map(murray_il.trans2, stripWhitespace)
murray_il_dtm<-DocumentTermMatrix(murray_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
murray_il_LDA2 <- LDA(murray_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_murray_il<-topics(murray_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_murray_il
most_freq_topic_murray_il<-which.max(tabulate(topics1_murray_il))
topic_terms_murray_il<-as.data.frame(terms(murray_il_LDA2, 10))
terms2_murray_il<-terms(murray_il_LDA2, 5)
terms2_murray_il
theta_murray_il<-as.data.frame(posterior(murray_il_LDA2)$topics)
theta_murray_il
topics2_murray_il
topics2_murray_il<-topics(murray_il_LDA2, 2)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
murray_il_LDA2_json<-topicmodels_json_ldavis(murray_il_LDA2, murray_il, murray_il_dtm)
serVis(murray_il_LDA2_json)
View(theta_pers_il)
View(theta_pers_il)
View(s_theta_pers_il)
View(s_theta_pers_il)
s_theta_pers_il<-apply(theta_pers_il, 1, sort)
s_theta_murray_il<-apply(theta_murray_il, 1, sort)
Diff_pers_murray<-(sqrt(s_theta_pers_il)-sqrt(s_theta_murray_il))**2
HD_pers_murray0<-rowSums(Diff_pers_murray)
HD_pers_murray<-mean(HD_pers_murray0)
HD_pers_murray
pope_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Pope_1720"), readerControl=list(language="en"))
pope_il.trans1<-tm_map(pope_il, removeNumbers)
pope_il.trans2<-tm_map(pope_il.trans1, removePunctuation)
pope_il.trans3<-tm_map(pope_il.trans2, stripWhitespace)
pope_il_dtm<-DocumentTermMatrix(pope_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
pope_il_LDA2 <- LDA(pope_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pope_il<-topics(pope_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_pope_il
most_freq_topic_pope_il<-which.max(tabulate(topics1_pope_il))
topic_terms_pope_il<-as.data.frame(terms(pope_il_LDA2, 10))
terms2_pope_il<-terms(pope_il_LDA2, 5)
terms2_pope_il
theta_pope_il<-as.data.frame(posterior(pope_il_LDA2)$topics)
theta_pope_il
topics2_pope_il<-topics(pope_il_LDA2, 2)
topics2_pope_il
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
pope_il_LDA2_json<-topicmodels_json_ldavis(pope_il_LDA2, pope_il, pope_il_dtm)
serVis(pope_il_LDA2_json)
s_theta_pers_il<-apply(theta_pers_il, 1, sort)
s_theta_pope_il<-apply(theta_pope_il, 1, sort)
Diff_pers_pope<-(sqrt(s_theta_pers_il)-sqrt(s_theta_pope_il))**2
HD_pers_pope0<-rowSums(Diff_pers_pope)
HD_pers_pope<-mean(HD_pers_pope0)
HD_pers_pope
chapman_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Chapman_1598"), readerControl=list(language="en"))
chapman_il.trans1<-tm_map(chapman_il, removeNumbers)
chapman_il.trans2<-tm_map(chapman_il.trans1, removePunctuation)
chapman_il.trans3<-tm_map(chapman_il.trans2, stripWhitespace)
chapman_il_dtm<-DocumentTermMatrix(chapman_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
chapman_il_LDA2 <- LDA(chapman_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_chapman_il<-topics(chapman_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_chapman_il
most_freq_topic_chapman_il<-which.max(tabulate(topics1_chapman_il))
topic_terms_chapman_il<-as.data.frame(terms(chapman_il_LDA2, 10))
terms2_chapman_il<-terms(chapman_il_LDA2, 5)
terms2_chapman_il
theta_chapman_il<-as.data.frame(posterior(chapman_il_LDA2)$topics)
theta_chapman_il
topics2_chapman_il<-topics(chapman_il_LDA2, 2)
topics2_chapman_il
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
chapman_il_LDA2_json<-topicmodels_json_ldavis(chapman_il_LDA2, chapman_il, chapman_il_dtm)
serVis(chapman_il_LDA2_json)
s_theta_chapman_il<-apply(theta_chapman_il, 1, sort)
Diff_pers_chapman<-(sqrt(s_theta_pers_il)-sqrt(s_theta_chapman_il))**2
HD_pers_chapman0<-rowSums(Diff_pers_chapman)
HD_pers_chapman<-mean(HD_pers_chapman0)
HD_pers_chapman
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
chapman_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Johann_Heinrich_Vos_1793"), readerControl=list(language="de"))
chapman_il.trans1<-tm_map(chapman_il, removeNumbers)
chapman_il.trans2<-tm_map(chapman_il.trans1, removePunctuation)
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
vos_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Johann_Heinrich_Vos_1793"), readerControl=list(language="de"))
vos_il.trans1<-tm_map(vos_il, removeNumbers)
vos_il.trans2<-tm_map(vos_il.trans1, removePunctuation)
vos_il.trans3<-tm_map(vos_il.trans2, stripWhitespace)
vos_il_dtm<-DocumentTermMatrix(vos_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
vos_il_LDA2 <- LDA(vos_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_vos_il<-topics(vos_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_vos_il
most_freq_topic_vos_il<-which.max(tabulate(topics1_vos_il))
topic_terms_vos_il<-as.data.frame(terms(vos_il_LDA2, 10))
terms2_vos_il<-terms(vos_il_LDA2, 5)
terms2_vos_il
theta_vos_il<-as.data.frame(posterior(vos_il_LDA2)$topics)
theta_vos_il
topics2_vos_il<-topics(vos_il_LDA2, 2)
topics2_vos_il
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
vos_il_LDA2_json<-topicmodels_json_ldavis(vos_il_LDA2, vos_il, vos_il_dtm)
serVis(vos_il_LDA2_json)
chapman_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Chapman_1598"), readerControl=list(language="en"))
chapman_il.trans1<-tm_map(chapman_il, removeNumbers)
chapman_il.trans2<-tm_map(chapman_il.trans1, removePunctuation)
chapman_il.trans3<-tm_map(chapman_il.trans2, stripWhitespace)
chapman_il_dtm<-DocumentTermMatrix(chapman_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
chapman_il_LDA2 <- LDA(chapman_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_chapman_il<-topics(chapman_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_chapman_il
s_theta_pers_il<-t(apply(theta_pers_il, 1, sort, decreasing = TRUE))
s_theta_vos_il<-t(apply(theta_vos_il, 1, sort, decreasing = TRUE))
Diff_pers_vos<-(sqrt(s_theta_pers_il)-sqrt(s_theta_vos_il))**2
HD_pers_vos0<-rowSums(Diff_pers_wout13_vos)
HD_pers_vos<-mean(HD_pers_vos0)
s_theta_pers_il<-t(apply(theta_pers_il, 1, sort, decreasing = TRUE))
s_theta_vos_il<-t(apply(theta_vos_il, 1, sort, decreasing = TRUE))
Diff_pers_vos<-(sqrt(s_theta_pers_il)-sqrt(s_theta_vos_il))**2
HD_pers_vos0<-rowSums(Diff_pers_vos)
HD_pers_vos<-mean(HD_pers_vos0)
HD_pers_vos
View(s_theta_pers_il)
