# harmonic mean method is used to determine optimal k
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
#Set Topic NUMBER
#Example of the harmonic mean for one value of k, using a burn in of 1000 and iterating 1000 times
k <- 3
burnin <- 1000
iter <- 1000
keep <- 50
# log-likelihood values are then determined by first fitting the model.
#This returns all log-likelihood values including burn-in,
#i.e., these need to be omitted before calculating the harmonic mean:
pers_il_LDA <- LDA(pers_il_dtm, k = k, method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
## assuming that burnin is a multiple of keep
logLiks_pers_il <- pers_il_LDA@logLiks[-c(1:(burnin/keep))]
pers_il_LDA
#To find the optimal value for k for our corpus,
#we repeat the log-lik, harmonic mean calculations over a sequence of topic models with different vales for k.
#This will generate numerous topic models with different numbers of topics, creating a vector to hold the k values.
library(Rmpfr)
library(gmp)
harmonicMean(logLiks_pers_il)
#sequence of numbers from 2 to 100, stepped by one
seqk <- seq(2, 15, 1)
seqk
burnin <- 1000
iter <- 1000
keep <- 50
#Run the LDA function using all the values of k, using the lapply function
system.time(fitted_many <- lapply(seqk, function(k) LDA(pers_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin,
iter = iter, keep = keep) )))
CmltvFreq<-colSums(as.matrix(pers_il_dtm))
CmltvFreq[1:5]
dictionary_pers_il[1:5]
wordFreqs_pers_il<-sort(colSums(as.matrix(pers_il_dtm)), decreasing=TRUE)
wordFreqs_pers_il[1:5]
names(wordFreqs_pers_il)[1:5]
MostFreqNames<-names(wordFreqs_pers_il)[1:5]
hist(wordFreqs_pers_il)
wordFreqs_pers_il[1:50]
wordFreqs_pers_il[1:100]
wordcloud(words=names(wordFreqs_pers_il), freq=wordFreqs_pers_il)
MostFreqNames<-(wordFreqs_pers_il)[1:100]
MostFreqNames
wordcloud(words=names(MostFreqNames), freq=MostFreqNames)
warning()
wordFreqs_pers_il[1:1000]
MostFreqNames<-(wordFreqs_pers_il)[1:1000]
MostFreqNames
wordcloud(words=names(MostFreqNames), freq=MostFreqNames)
library(slam)
tfidf_pers_il <- tapply(pers_il_dtm$v / row_sums(pers_il_dtm)[pers_il_dtm$i], pers_il_dtm$j, mean) *
log2(nDocs(pers_il_dtm)/col_sums(pers_il_dtm > 0))
summary(tfidf_pers_il)
is.vector(tfidf_pers_il)
tfidf_pers_il[1:5]
length(tfidf_pers_il)
dim(tfidf_pers_il)
dictionary_pers_il[1:5]
reduced_dtm_pers_il <- pers_il_dtm[,tfidf_pers_il >= 0.001]
inspect(reduced_dtm_pers_il)
harmonicMean <- function(logLikelihoods, precision = 2000L) {
llMed <- median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
k <- 5
burnin <- 1000
iter <- 1000
keep <- 50
pers_il_LDA<-LDA(pers_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
pers_il_LDA
str(pers_il_LDA)
logLiks_pers_il<-pers_il_LDA@logLiks[-c(1:(burnin/keep))]
logLiks_pers_il
library(Rmpfr)
library(gmp)
harmonicMean(logLiks_pers_il)
seqk<-seq(2, 15, 1)
seqk
burnin<-1000
iter<-1000
keep<-50
system.time(fitted_many<-lapply(seqk, function(k) LDA(pers_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin,
iter = iter, keep = keep) )))
pers_il_LDA@logLiks
logLiks_many<-lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
hm_many<-sapply(logLiks_many, function(h) harmonicMean(h))
hm_many
which.max(hm_many)
seqk[which.max(hm_many)]
library(ggplot2)
seqk
plot(seqk, hm_many)
hm_many
??which.max
max(hm_many)
library(ggplot2)
ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
xlab('Number of Topics') +
ylab('Harmonic Mean') +
annotate("text", x = 5, y = -10000,
label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
ggtitle(expression(atop("Latent Dirichlet Allocation Analysis De Motu Animalium", atop(italic("How many distinct topics in De Motu?!"), ""))))
plot(seqk, hm_many)
ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
xlab('Number of Topics') +
ylab('Harmonic Mean') +
annotate("text", x = 5, y = -10000,
label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
ggtitle(expression(atop("Latent Dirichlet Allocation Analysis De Motu Animalium", atop(italic("How many distinct topics in De Motu?!"), ""))))
topics1_pers_il<-topics(pers_il_LDA2, 1) # 1 = number of topics ranked by likelihood
pers_il_LDA1 <- LDA(pers_il_dtm, k = 3, method = "Gibbs", control = list(iter=2000, seed = 0622))
pers_il_LDA2 <- LDA(pers_il_dtm, k = 11, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pers_il<-topics(pers_il_LDA2, 1) # 1 = number of topics ranked by likelihood
pers_il_LDA1 <- LDA(pers_il_dtm, k = 15, method = "Gibbs", control = list(iter=2000, seed = 0622))
pers_il_LDA2 <- LDA(pers_il_dtm, k = 25, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pers_il<-topics(pers_il_LDA2, 1) # 1 = number of topics ranked by likelihood
topics1_pers_il
topics1_pers_il<-topics(pers_il_LDA2, 2) # 1 = number of topics ranked by likelihood
topics1_pers_il
most_freq_topic<-which.max(tabulate(topics1_pers_il))
topic_terms<-as.data.frame(terms(pers_il_LDA2, 10))
View(topics1_pers_il)
View(topics1_pers_il)
View(topic_terms)
View(topic_terms)
terms1<-terms(pers_il_LDA1, 5)
terms1
terms2<-terms(pers_il_LDA2, 5)
terms2
theta<-as.data.frame(posterior(pers_il_LDA2)$topics)
theta
topics2<-topics(pers_il_LDA2, 2)
topics2
topics3<-topics(pers_il_LDA1, 2)
View(theta)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
pers_il_LDA2_json<-topicmodels_json_ldavis(pers_il_LDA2, pers_il, pers_il_dtm)
serVis(pers_il_LDA2_json)
harmonicMean(logLiks_pers_il)
k<-25
burnin<-1000
iter<-1000
keep<-50
pers_il_LDA<-LDA(pers_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
pers_il_LDA
str(pers_il_LDA)
logLiks_pers_il<-pers_il_LDA@logLiks[-c(1:(burnin/keep))]
pers_il_LDA@logLiks
library(Rmpfr)
library(gmp)
harmonicMean(logLiks_pers_il)
seqk<-seq(2, 100, 1)
seqk
burnin<-1000
iter<-1000
keep<-50
system.time(fitted_many<-lapply(seqk, function(k) LDA(pers_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin,
iter = iter, keep = keep) )))
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/Polylas_1890_iliad"), readerControl=list(language="el"))
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/Polylas_1890_iliad"), readerControl=list(language="el"))
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/Polylas_1890_iliad"), readerControl=list(language="el"))
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/perseus_iliad"), readerControl=list(language="el"))
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/perseus_iliad"), readerControl=list(language="el"))
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics-1/perseus_iliad"), readerControl=list(language="el"))
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics/perseus_iliad"), readerControl=list(language="el"))
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/perseus_iliad"), readerControl=list(language="el"))
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Polylas_1890_iliad"), readerControl=list(language="el"))
inspect(polylas_il[[1]])
print(polylas_il[[1]])
meta(polylas_il[[1]])
str(polylas_il)
polylas_il.trans1<-tm_map(polylas_il, removeNumbers)
inspect(polylas_il.trans1[[1]])
polylas_il.trans2<-tm_map(polylas_il.trans1, removePunctuation)
inspect(polylas_il.trans2[[1]])
polylas_il.trans3<-tm_map(polylas_il.trans2, stripWhitespace)
inspect(polylas_il.trans3[[1]])
polylas_il_dtm<-DocumentTermMatrix(polylas_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
inspect(polylas_il_dtm)
polylas_il_dtm$v[1:3]
polylas_il_dtm$dimnames$Terms[1:3] #Corpus dictionary: length = 19763
findFreqTerms(polylas_il_dtm, 50) #List of words appearing more than 50 times
dim(polylas_il_dtm)
str(polylas_il_dtm)
nrow(polylas_il_dtm) #number of documents (i.e. books of the Iliad)
ncol(polylas_il_dtm) #number of terms
rownames(polylas_il_dtm)
colnames(polylas_il_dtm) #Alternatively: polylas_il$dimnames$Terms
dictionary_polylas_il<-colnames(polylas_il_dtm)
dictionary_polylas_il
length(dictionary_polylas_il)
library(RColorBrewer)
library(wordcloud)
UnsortedFreq<-colSums(as.matrix(polylas_il_dtm))
UnsortedFreq[1:5]
dictionary_polylas_il[1:5]
wordFreqs_polylas_il<-sort(colSums(as.matrix(polylas_il_dtm)), decreasing=TRUE)
wordFreqs_polylas_il[1:1000]
names(wordFreqs_polylas_il)[1:100]
MostFreqNames<-(wordFreqs_polylas_il)[1:1000]
MostFreqNames
MostFreqNames<-(wordFreqs_polylas_il)[1:100]
MostFreqNames
wordcloud(words=names(MostFreqNames), freq=MostFreqNames)
library(slam)
tfidf_polylas_il<-tapply(polylas_il_dtm$v / row_sums(polylas_il_dtm)[polylas_il_dtm$i], polylas_il_dtm$j, mean) *
log2(nDocs(polylas_il_dtm)/col_sums(polylas_il_dtm > 0))
is.vector(tfidf_polylas_il) #not a vector
tfidf_polylas_il[1:5]
dictionary_polylas_il[1:5]
length(tfidf_polylas_il)
dim(tfidf_polylas_il)
summary(tfidf_polylas_il)
reduced_dtm_polylas_il<-polylas_il_dtm[,tfidf_polylas_il >= 0.001]
summary(col_sums(reduced_dtm_polylas_il))
inspect(reduced_dtm_polylas_il)
dim(reduced_dtm_polylas_il)
findFreqTerms(reduced_dtm_polylas_il, 5) # at least 5 times
wordFreqs_polylas_il<-sort(colSums(as.matrix(reduced_dtm_polylas_il)), decreasing=TRUE)
wordcloud(words=names(wordFreqs_polylas_il), freq=wordFreqs_polylas_il)
harmonicMean<-function(logLikelihoods, precision = 2000L) {
llMed<-median(logLikelihoods)
as.double(llMed - log(mean(exp(-mpfr(logLikelihoods,
prec = precision) + llMed))))
}
k<-25
burnin<-1000
iter<-1000
keep<-50
polylas_il_LDA<-LDA(polylas_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin, iter = iter, keep = keep) )
polylas_il_LDA
str(polylas_il_LDA)
logLiks_polylas_il<-polylas_il_LDA@logLiks[-c(1:(burnin/keep))]
polylas_il_LDA@logLiks
library(Rmpfr)
library(gmp)
harmonicMean(logLiks_polylas_il)
seqk<-seq(2, 50, 1)
seqk
burnin<-1000
iter<-1000
keep<-50
system.time(fitted_many<-lapply(seqk, function(k) LDA(polylas_il_dtm, k = k,
method = "Gibbs",control = list(burnin = burnin,
iter = iter, keep = keep) )))
logLiks_many<-lapply(fitted_many, function(L)  L@logLiks[-c(1:(burnin/keep))])
hm_many<-sapply(logLiks_many, function(h) harmonicMean(h))
hm_many
??which.max
which.max(hm_many)
max(hm_many)
seqk
seqk[which.max(hm_many)]
library(ggplot2)
plot(seqk, hm_many)
ggplot(data.frame(seqk, hm_many), aes(x=seqk, y=hm_many)) + geom_path(lwd=1.5) +
xlab('Number of Topics') +
ylab('Harmonic Mean') +
annotate("text", x = 5, y = -10000,
label = paste("The optimal number of topics is", seqk[which.max(hm_many)])) +
ggtitle(expression(atop("polylas_il", atop(italic("How many distinct topics"), ""))))
polylas_il_LDA1 <- LDA(polylas_il_dtm, k = 15, method = "Gibbs", control = list(iter=2000, seed = 0622))
polylas_il_LDA1 <- LDA(polylas_il_dtm, k = 15, method = "Gibbs", control = list(iter=2000, seed = 0622))
polylas_il_LDA2 <- LDA(polylas_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_polylas_il<-topics(polylas_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_polylas_il
most_freq_topic<-which.max(tabulate(topics1_polylas_il))
View(topics1_polylas_il)
View(topics1_polylas_il)
topic_terms<-as.data.frame(terms(polylas_il_LDA2, 10))
terms1<-terms(polylas_il_LDA1, 5)
terms1
terms2<-terms(polylas_il_LDA2, 5)
terms2
theta<-as.data.frame(posterior(polylas_il_LDA2)$topics)
theta
View(theta)
View(terms1)
View(terms1)
View(terms2)
View(terms2)
View(terms1)
View(terms1)
View(terms2)
View(terms2)
topics2<-topics(polylas_il_LDA2, 2)
topics2
topics3<-topics(polylas_il_LDA1, 2)
topics3
View(topics2)
View(topics2)
pers_il_LDA1 <- LDA(pers_il_dtm, k = 15, method = "Gibbs", control = list(iter=2000, seed = 0622))
pers_il_LDA2 <- LDA(pers_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pers_il<-topics(pers_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_pers_il
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
pers_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/perseus_iliad"), readerControl=list(language="el"))
inspect(pers_il[[1]])
print(pers_il[[1]])
meta(pers_il[[1]])
str(pers_il)
pers_il.trans1<-tm_map(pers_il, removeNumbers)
inspect(pers_il.trans1[[1]])
pers_il.trans2<-tm_map(pers_il.trans1, removePunctuation)
inspect(pers_il.trans2[[1]])
pers_il.trans3<-tm_map(pers_il.trans2, stripWhitespace)
inspect(pers_il.trans3[[1]])
pers_il_dtm<-DocumentTermMatrix(pers_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
pers_il_LDA1 <- LDA(pers_il_dtm, k = 15, method = "Gibbs", control = list(iter=2000, seed = 0622))
pers_il_LDA2 <- LDA(pers_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pers_il<-topics(pers_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_pers_il
most_freq_topic<-which.max(tabulate(topics1_pers_il))
topic_terms<-as.data.frame(terms(pers_il_LDA2, 10))
terms1<-terms(pers_il_LDA1, 5)
terms1
terms2<-terms(pers_il_LDA2, 5)
terms2
theta<-as.data.frame(posterior(pers_il_LDA2)$topics)
theta
theta_pers_il<-as.data.frame(posterior(pers_il_LDA2)$topics)
theta_pers_il
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
polylas_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Polylas_1890_iliad"), readerControl=list(language="el"))
inspect(polylas_il[[1]])
pallis_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Pallis_1936_missing_bk13"), readerControl=list(language="el"))
inspect(pallis_il[[1]])
print(pallis_il[[1]])
meta(pallis_il[[1]])
str(pallis_il)
pallis_il.trans1<-tm_map(pallis_il, removeNumbers)
inspect(pallis_il.trans1[[1]])
pallis_il.trans2<-tm_map(pallis_il.trans1, removePunctuation)
inspect(pallis_il.trans2[[1]])
pallis_il.trans3<-tm_map(pallis_il.trans2, stripWhitespace)
inspect(pallis_il.trans3[[1]])
pallis_il_dtm<-DocumentTermMatrix(pallis_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
inspect(pallis_il_dtm)
pallis_il_dtm$v[1:3]
pallis_il_dtm$dimnames$Terms[1:3] #Corpus dictionary: length = ?
pallis_il_LDA1 <- LDA(pallis_il_dtm, k = 25, method = "Gibbs", control = list(iter=2000, seed = 0622))
pallis_il_LDA2 <- LDA(pallis_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_pallis_il<-topics(pallis_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_pallis_il
most_freq_topic_pallis_il<-which.max(tabulate(topics1_pallis_il))
topic_terms_pallis_il<-as.data.frame(terms(pallis_il_LDA2, 10))
terms1_pallis_il<-terms(pallis_il_LDA1, 5)
terms1_pallis_il
terms2_pallis_il<-terms(pallis_il_LDA2, 5)
terms2_pallis_il
theta_pallis_il<-as.data.frame(posterior(pallis_il_LDA2)$topics)
theta_pallis_il
topics2_pallis_il<-topics(pallis_il_LDA2, 2)
topics2_pallis_il
topics3_pallis_il<-topics(pallis_il_LDA1, 2)
topics3_pallis_il
junk0 <- as.matrix(pallis_il_dtm)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
pallis_il_LDA2_json<-topicmodels_json_ldavis(pallis_il_LDA2, pallis_il, pallis_il_dtm)
serVis(pallis_il_LDA2_json)
library(NLP)
library(topicmodels)
library(dplyr)
library(stringi)
library(tm)
library(LDAvis)
library(RColorBrewer)
library(wordcloud)
library(Rmpfr)
library(gmp)
library(slam)
kazatzakis_il<-VCorpus(DirSource("/Users/sofia/homer_topics_R_2018/Kazatzakis"), readerControl=list(language="el"))
inspect(kazatzakis_il[[1]])
meta(kazatzakis_il[[1]])
str(kazatzakis_il)
kazatzakis_il.trans1<-tm_map(kazatzakis_il, removeNumbers)
inspect(kazatzakis_il.trans1[[1]])
kazatzakis_il.trans2<-tm_map(kazatzakis_il.trans1, removePunctuation)
inspect(kazatzakis_il.trans2[[1]])
kazatzakis_il.trans3<-tm_map(kazatzakis_il.trans2, stripWhitespace)
inspect(kazatzakis_il.trans3[[1]])
kazatzakis_il_dtm<-DocumentTermMatrix(kazatzakis_il,
control= list(stemming=FALSE, wordLengths = c(5, Inf),
removeNumbers=TRUE, removePunctuation=TRUE))
inspect(kazatzakis_il_dtm)
kazatzakis_il_dtm$v[1:3]
kazatzakis_il_dtm$dimnames$Terms[1:3] #Corpus dictionary: length = ?
kazatzakis_il_LDA1 <- LDA(kazatzakis_il_dtm, k = 25, method = "Gibbs", control = list(iter=2000, seed = 0622))
kazatzakis_il_LDA2 <- LDA(kazatzakis_il_dtm, k = 10, method = "Gibbs", control = list(iter=2000, seed = 0622))
topics1_kazatzakis_il<-topics(kazatzakis_il_LDA2, 2) # 2 = number of topics ranked by likelihood
topics1_kazatzakis_il
most_freq_topic_kazatzakis_il<-which.max(tabulate(topics1_kazatzakis_il))
topic_terms_kazatzakis_il<-as.data.frame(terms(kazatzakis_il_LDA2, 10))
terms1_kazatzakis_il<-terms(kazatzakis_il_LDA1, 5)
terms1_kazatzakis_il
terms2_kazatzakis_il<-terms(kazatzakis_il_LDA2, 5)
terms2_kazatzakis_il
theta_kazatzakis_il<-as.data.frame(posterior(kazatzakis_il_LDA2)$topics)
theta_kazatzakis_il
topics2_kazatzakis_il<-topics(kazatzakis_il_LDA2, 2)
topics2_kazatzakis_il
topics3_kazatzakis_il<-topics(kazatzakis_il_LDA1, 2)
topics3_kazatzakis_il
junk0 <- as.matrix(kazatzakis_il_dtm)
topicmodels_json_ldavis <- function(fitted, corpus, doc_term){
## Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- vector()
for (i in 1:length(corpus)) {
temp <- paste(corpus[[i]]$content, collapse = ' ')
doc_length <- c(doc_length, stri_count(temp, regex = '\\S+'))
}
temp_frequency <- as.matrix(doc_term)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
## Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
kazatzakis_il_LDA2_json<-topicmodels_json_ldavis(kazatzakis_il_LDA2, kazatzakis_il, kazatzakis_il_dtm)
serVis(kazatzakis_il_LDA2_json)
View(theta_kazatzakis_il)
